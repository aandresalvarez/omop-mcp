# Stage 3: BigQuery SQL Generation

**Pydantic AI implementation for generating validated BigQuery SQL from OMOP cohort definitions.**

---

## ğŸ“‹ Overview

This is **Stage 3** of the complete OMOP cohort workflow. It takes the clinical definition (Stage 1) and OMOP concept sets (Stage 2) and generates a validated BigQuery SQL query that can be executed against an OMOP CDM database.

---

## ğŸ”„ Workflow Position

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 1:       â”‚  User describes cohort
â”‚   Clarification  â”‚  â†’ Structured clinical definition
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 2:       â”‚  Clinical text â†’ OMOP concepts
â”‚   Concept        â”‚  â†’ Standard concept IDs
â”‚   Discovery      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 3:       â”‚  â—„â”€â”€ YOU ARE HERE
â”‚   BigQuery SQL   â”‚  Cohort + concepts â†’ BigQuery SQL
â”‚   Generation     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
     Execute in BigQuery
```

---

## âš™ï¸ How It Works

### 1. **Input**

Reads from: `projects/run/complete_cohort_output.json`

```json
{
  "clinical_definition": {
    "index_event": "positive flu test",
    "demographics": {"age": "20-30", "gender": "male"},
    "observation_window": "in year 2020"
  },
  "concept_sets": [
    {
      "name": "Influenza Test",
      "included_concepts": [
        {"concept_id": 4171852, "concept_name": "Influenza virus A RNA"}
      ],
      "include_descendants": true
    }
  ]
}
```

### 2. **SQL Generation Agent**

- **Model**: `gpt-4o-mini`
- **Task**: Generate BigQuery Standard SQL for OMOP CDM v5.x
- **Output**: Complete SELECT statement with CTEs

**Rules enforced**:
- Fully qualified table names (`project.dataset.table`)
- OMOP CDM v5.x field names
- Concept ID filtering
- Descendant expansion via `concept_ancestor`
- Standard concepts only (`standard_concept='S'`)

### 3. **Validation Loop**

- **BigQuery Dry Run**: Validates SQL syntax without executing
- **Cost Estimation**: Estimates bytes processed and USD cost
- **Auto-Fix**: If validation fails, SQL Fixer agent corrects errors

### 4. **Output**

```
projects/qb/
  â”œâ”€â”€ generated_cohort_query.sql    â† Validated SQL
  â””â”€â”€ sql_generation_result.json    â† Validation metadata
```

---

## ğŸš€ Usage

### **Quick Start**

```bash
cd /Users/alvaro1/Documents/Coral/Code/cohortgen/projects/qb
./run_query_builder.sh
```

### **Standalone Python**

```bash
cd /Users/alvaro1/Documents/Coral/Code/cohortgen
uv run python projects/qb/create_bigquery_sql.py
```

### **Prerequisites**

1. **Stage 1 + Stage 2 complete**: `projects/run/complete_cohort_output.json` must exist
2. **Environment variables** (optional):
   ```bash
   OMOP_DATASET_ID="bigquery-public-data.cms_synthetic_patient_data_omop"
   BIGQUERY_PROJECT_ID="your-gcp-project"
   BIGQUERY_LOCATION="US"
   OPENAI_API_KEY="sk-..."
   ```

---

## ğŸ“ Example Output

### **Generated SQL**

```sql
WITH cohort_base AS (
  SELECT DISTINCT m.person_id
  FROM `bigquery-public-data.cms_synthetic_patient_data_omop.measurement` m
  WHERE m.measurement_concept_id IN (4171852, 4171853)
    AND EXTRACT(YEAR FROM m.measurement_date) = 2020
),
demographics_filter AS (
  SELECT p.person_id
  FROM `bigquery-public-data.cms_synthetic_patient_data_omop.person` p
  WHERE p.gender_concept_id = 8507  -- Male
    AND EXTRACT(YEAR FROM CURRENT_DATE()) - p.year_of_birth BETWEEN 20 AND 30
)
SELECT DISTINCT cb.person_id
FROM cohort_base cb
INNER JOIN demographics_filter df ON cb.person_id = df.person_id;
```

### **Validation Result**

```json
{
  "sql": "WITH cohort_base AS (...",
  "is_valid": true,
  "errors": [],
  "estimated_cost_usd": 0.0012,
  "total_bytes_processed": 204800,
  "summary": "Query is valid. Estimated to process 0.20 GB, costing ~$0.0012."
}
```

---

## ğŸ—ï¸ Architecture

### **Components**

1. **`tools.py`**: Pydantic AI wrapper for `bq_dry_run`
   - Validates SQL syntax
   - Estimates query cost
   - Returns structured results

2. **`create_bigquery_sql.py`**: Main workflow script
   - SQL Generator Agent
   - SQL Fixer Agent
   - Validation loop
   - File I/O

3. **`run_query_builder.sh`**: Shell script for easy execution

### **Agents**

#### **SQL Generator Agent**
- **Input**: Clinical definition + concept sets + OMOP dataset
- **Output**: BigQuery Standard SQL
- **System Prompt**: Expert OMOP CDM SQL developer with BigQuery focus

#### **SQL Fixer Agent**
- **Input**: Original SQL + BigQuery error messages
- **Output**: Corrected SQL
- **System Prompt**: SQL debugger that preserves logic and fixes syntax

### **Validation Loop**

```python
for iteration in range(max_fix_iterations):
    result = validate_bigquery_sql(current_sql)
    if result.success:
        return result  # âœ… Valid SQL
    # âŒ Has errors - fix it
    current_sql = sql_fixer_agent.run_sync(fix_prompt)
```

**Max iterations**: 3 (configurable)

---

## ğŸ†š Comparison: Flujo vs Pydantic AI

| Feature | Flujo Version | Pydantic AI Version |
|---------|---------------|---------------------|
| **File structure** | `pipeline.yaml` + `skills/*.py` | `create_bigquery_sql.py` + `tools.py` |
| **Agents** | Declarative YAML config | Python `Agent()` objects |
| **Validation loop** | `loop` step with `exit_expression` | Python `for` loop |
| **Tools** | Custom skills via `uses:` | Pydantic AI `tools=[]` |
| **Error handling** | Conditional steps | Python try/except |
| **State management** | Context + scratchpad | Python variables |
| **HITL** | Built-in `kind: hitl` | Not needed (fully automated) |

---

## ğŸ”§ Configuration

### **Environment Variables**

Set in `/.env` or export before running:

```bash
# Required
OPENAI_API_KEY="sk-..."

# Optional (defaults shown)
OMOP_DATASET_ID="bigquery-public-data.cms_synthetic_patient_data_omop"
BIGQUERY_PROJECT_ID=""  # Empty = use default project
BIGQUERY_LOCATION="US"
```

### **OMOP Dataset**

Default: **BigQuery Public Dataset** (`bigquery-public-data.cms_synthetic_patient_data_omop`)
- Synthetic Medicare claims data
- OMOP CDM v5.x
- Free to query (standard BigQuery pricing applies)

**Custom dataset**: Set `OMOP_DATASET_ID` to your dataset path.

---

## ğŸ“š Integration with Full Workflow

### **Run All 3 Stages**

```bash
cd /Users/alvaro1/Documents/Coral/Code/cohortgen/projects/run
./run_integrated.sh   # Stage 1 + 2

cd /Users/alvaro1/Documents/Coral/Code/cohortgen/projects/qb
./run_query_builder.sh  # Stage 3
```

### **Data Flow**

```
Stage 1: hitl_clarification_working.py
  â†“ (CohortDefinition object)

Stage 2: find_concepts.py
  â†“ (ConceptDiscoveryOutput object)

run_complete_workflow.py combines them
  â†“ (complete_cohort_output.json)

Stage 3: create_bigquery_sql.py
  â†“ (generated_cohort_query.sql)

Execute in BigQuery
```

---

## ğŸ§ª Testing

### **Test with Sample Data**

```bash
# 1. Generate sample cohort (Stage 1 + 2)
cd projects/run
./run_integrated.sh
# Input: "male between 20 and 30 with positive flu test in 2020"

# 2. Generate SQL (Stage 3)
cd ../qb
./run_query_builder.sh

# 3. Review output
cat generated_cohort_query.sql
```

### **Validate Manually**

```bash
# Copy SQL to BigQuery console
# Click "More" â†’ "Query Settings"
# Enable "Dry run" to validate without executing
```

---

## ğŸ› Troubleshooting

### **Error: "Input file not found"**

**Cause**: Stage 1 + 2 haven't been run yet.

**Fix**:
```bash
cd /Users/alvaro1/Documents/Coral/Code/cohortgen/projects/run
./run_integrated.sh
```

### **Error: "google-cloud-bigquery is not installed"**

**Cause**: BigQuery Python client not installed.

**Fix**:
```bash
uv add google-cloud-bigquery
```

### **Error: "SQL validation failed after 3 attempts"**

**Cause**: Generated SQL has persistent syntax errors.

**Fix**: Review `sql_generation_result.json` for error details. Common issues:
- Wrong table names (check `OMOP_DATASET_ID`)
- Missing fields (verify OMOP CDM version)
- Invalid concept IDs

---

## ğŸ“Š Output Files

### **`generated_cohort_query.sql`**

- **Purpose**: Ready-to-execute BigQuery SQL
- **Format**: Plain SQL (no markdown)
- **Usage**: Copy-paste into BigQuery console or use via API

### **`sql_generation_result.json`**

- **Purpose**: Validation metadata and cost estimates
- **Format**: JSON
- **Contents**:
  ```json
  {
    "sql": "WITH cohort_base AS ...",
    "is_valid": true,
    "errors": [],
    "estimated_cost_usd": 0.0012,
    "total_bytes_processed": 204800,
    "summary": "Query is valid. Estimated to process 0.20 GB..."
  }
  ```

---

## ğŸ¯ Summary

| **Aspect** | **Details** |
|------------|-------------|
| **Purpose** | Generate validated BigQuery SQL for OMOP cohorts |
| **Input** | `complete_cohort_output.json` (from Stage 1 + 2) |
| **Output** | `generated_cohort_query.sql` + metadata |
| **Framework** | Pydantic AI |
| **Model** | `gpt-4o-mini` |
| **Validation** | BigQuery dry run (syntax + cost) |
| **Auto-fix** | Yes (up to 3 iterations) |
| **HITL** | No (fully automated) |

---

## ğŸš€ Next Steps

1. **Review SQL**: Check `generated_cohort_query.sql`
2. **Execute**: Run in BigQuery console or via API
3. **Analyze**: Export results and perform statistical analysis
4. **Iterate**: Refine cohort definition and regenerate

---

## ğŸ“– Related Documentation

- **Stage 1**: `projects/clar/README.md`
- **Stage 2**: `projects/cd/README.md`
- **Full Workflow**: `HOW_TO_RUN.md`
- **Flujo Version**: `projects/query_builder/README.md`
